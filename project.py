# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1286ov0YxraZcJqUb4aZCpFWsls27SvkV
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
# To ignore warnings
import warnings
warnings.filterwarnings("ignore")
import matplotlib.ticker as mtick

!pip install xgboost
!pip install lightgbm
!pip install catboost
!pip install folium

df = pd.read_csv('House_Rent_Dataset.csv')
df.head()

df.shape

df.info()

df.describe()

df.isnull().sum()

"""# Exploratory Data Analysis (EDA)"""

df['City'].unique()

df['Furnishing Status'].unique()

df['Area Type'].unique()

df['Point of Contact'].unique()

df['Tenant Preferred'].unique()

df.value_counts(df['Tenant Preferred'])

df.value_counts(df['City'])

df.value_counts(df['Point of Contact'])

df.value_counts(df['Area Type'])

df.value_counts(df['Furnishing Status'])

sns.pairplot(df)

df.columns

df.duplicated(subset=['Latitude', 'Longitude']).sum()

# Histogram for Rent distribution
plt.figure(figsize=(10, 5))
sns.histplot(df['Rent'], kde=True, color='red', bins=30)

# Set title and labels
plt.title("Distribution of Rent")
plt.xlabel("Rent")
plt.ylabel("Frequency")

# Format x-axis to avoid scientific notation
plt.ticklabel_format(style='plain', axis='x')

# Show the plot
plt.show()

# Bar plot for BHK distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='BHK', data=df, palette='icefire')
plt.title("BHK Distribution")
plt.xlabel("Number of Bedrooms")
plt.ylabel("Count")
plt.show()

# Bar plot for Area Type
plt.figure(figsize=(8, 5))
sns.countplot(x='Area Type', data=df, palette= 'icefire')
plt.title("Area Type Distribution")
plt.xlabel("Area Type")
plt.ylabel("Count")
plt.show()

# Bar plot for Furnishing Status
plt.figure(figsize=(8, 5))
sns.countplot(x='Furnishing Status', data=df, palette='Set2')
plt.title("Furnishing Status Distribution")
plt.xlabel("Furnishing Status")
plt.ylabel("Count")
plt.show()

# Bar plot for city
plt.figure(figsize=(8, 5))
sns.countplot(x='City', data=df, palette='icefire')  # Replace 'City' with the actual column name
plt.title("City Distribution") # Update title
plt.xlabel("City") # Update xlabel
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(8, 5))
plt.rcParams['axes.facecolor'] = 'white'
sns.scatterplot(x='Size', y='Rent', data=df, hue='BHK', palette="Set2")
plt.title("Rent Distribution by Size and BHK")
plt.xlabel("Size (sq ft)")
plt.ylabel("Rent")
plt.show()

# Select only numerical columns from the DataFrame
numerical_df = df.select_dtypes(include=['int64', 'float64'])

# Correlation matrix for numerical columns
plt.figure(figsize=(10, 6))
sns.heatmap(numerical_df.corr(), annot=True, cmap='Spectral', linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

# Extract floor number inline during plotting
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=df['Floor'].apply(lambda x: int(x.split(' ')[0]) if x.split(' ')[0].isdigit() else None),
    y=df['Rent']
)
plt.title("Rent vs Floor")
plt.xlabel("Floor Number")
plt.ylabel("Rent")
plt.show()

# Group by 'Tenant Preferred' and calculate the mean rent for each category
mean_rent_per_category = df.groupby('Tenant Preferred')['Rent'].mean()

# Display the results sorted by average rent in descending order
mean_rent_per_category = mean_rent_per_category.sort_values(ascending=False)
print(mean_rent_per_category)

"""# Map Are used to define location.
## Note
### 1)use one city at time, it take loot of time to show all l  
"""

#!pip install folium

import folium
mumbai_data = df[df['City'] == 'Mumbai']

# Get the center coordinates for Mumbai
mumbai_lat = mumbai_data['Latitude'].mean()
mumbai_lon = mumbai_data['Longitude'].mean()

# Create the map centered around Mumbai
mumbai_map = folium.Map(location=[mumbai_lat, mumbai_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in mumbai_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(mumbai_map)

# Display the map
mumbai_map

df['City'].unique()

import folium
Bangalore_data = df[df['City'] == 'Bangalore']

# Get the center coordinates for Mumbai
Bangalore_lat = Bangalore_data['Latitude'].mean()
Bangalore_lon = Bangalore_data['Longitude'].mean()

# Create the map centered around Mumbai
Bangalore_map = folium.Map(location=[Bangalore_lat, Bangalore_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in Bangalore_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(Bangalore_map)

# Display the map
Bangalore_map

import folium
Chennai_data = df[df['City'] == 'Chennai']

# Get the center coordinates for Mumbai
Chennai_lat = Chennai_data['Latitude'].mean()
Chennai_lon = Chennai_data['Longitude'].mean()

# Create the map centered around Mumbai
Chennai_map = folium.Map(location=[Chennai_lat, Chennai_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in Chennai_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(Chennai_map)

# Display the map
Chennai_map

import folium
Hyderabad_data = df[df['City'] == 'Hyderabad']

# Get the center coordinates for Mumbai
Hyderabad_lat = Hyderabad_data['Latitude'].mean()
Hyderabad_lon = Hyderabad_data['Longitude'].mean()

# Create the map centered around Mumbai
Hyderabad_map = folium.Map(location=[Hyderabad_lat, Hyderabad_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in Hyderabad_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(Hyderabad_map)

# Display the map
Hyderabad_map

import folium
Delhi_data = df[df['City'] == 'Delhi']

# Get the center coordinates for Mumbai
Delhi_lat = Delhi_data['Latitude'].mean()
Delhi_lon = Delhi_data['Longitude'].mean()

# Create the map centered around Mumbai
Delhi_map = folium.Map(location=[Delhi_lat, Delhi_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in Delhi_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(Delhi_map)

# Display the map
Delhi_map

import folium
Kolkata_data = df[df['City'] == 'Kolkata']

# Get the center coordinates for Mumbai
Kolkata_lat = Kolkata_data['Latitude'].mean()
Kolkata_lon = Kolkata_data['Longitude'].mean()

# Create the map centered around Mumbai
Kolkata_map = folium.Map(location=[Kolkata_lat, Kolkata_lon], zoom_start=12)

# Add markers for each location in mumbai_data
for index, row in Kolkata_data.iterrows():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=row['Area Locality']  # You can customize the popup content
    ).add_to(Kolkata_map)

# Display the map
Kolkata_map

"""#ðŸ”§ Feature Engineering ðŸ”§"""

# 1. Exclude irrelevant columns
columns_to_exclude = ['Posted On', 'Area Locality', 'Point of Contact', 'Latitude',	'Longitude']
df_filtered = df.drop(columns=columns_to_exclude)

df_filtered.head(20)

df_filtered['Tenant Preferred'].value_counts()

# Extract floor number from 'Floor' column
def extract_floor(floor_value):
    try:
        floor_info = floor_value.split(' ')[0]
        if floor_info.isdigit():
            return int(floor_info)
        elif floor_info.lower() in ['ground', 'lower']:
            return 0
        else:
            return None
    except:
        return None

df_filtered['Floor'] = df_filtered['Floor'].apply(extract_floor)
df_filtered.head(5)

df_filtered.isna().sum()

df_filtered = df_filtered.dropna()

df_filtered.isna().sum()

# Apply One-Hot Encoding with dtype=int to ensure binary 1s and 0s
df_encoded = pd.get_dummies(df_filtered, columns=['Area Type', 'City', 'Furnishing Status', 'Tenant Preferred'], drop_first=True, dtype=int)
df_encoded.head()

# Convert 'Floor' to integer
df_encoded['Floor'] = df_encoded['Floor'].astype(int)
df_encoded.head()

# Import the necessary class
from sklearn.preprocessing import StandardScaler


# Select the numerical columns to scale
columns_to_scale = ['BHK', 'Size', 'Floor', 'Bathroom']

# Initialize the scaler
scaler = StandardScaler()

# Scale the numerical columns
df_encoded[columns_to_scale] = scaler.fit_transform(df_encoded[columns_to_scale])

"""#ðŸš€ Model Creation ðŸš€


"""

# Separate features and target
X = df_encoded.drop('Rent', axis=1)
y = df_encoded['Rent']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""# Linear Regression Model"""

# Initialize the model
lr_model = LinearRegression()

# Train the model
lr_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred_lr = lr_model.predict(X_test)
print("Linear Regression R^2:", r2_score(y_test, y_pred_lr))
print("Linear Regression RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))

# Evaluate the model
lr_r2 = r2_score(y_test, y_pred_lr)
lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))
lr_mae = mean_absolute_error(y_test, y_pred_lr)

# Print evaluation metrics
print(f"Linear Regression RÂ²: {lr_r2:.3f}")
print(f"Linear Regression RMSE: {lr_rmse:.3f}")
print(f"Linear Regression MAE: {lr_mae:.3f}")

"""#Random Forest Model"""

# Initialize and train the Random Forest model with tuned hyperparameters
rf_model = RandomForestRegressor(
    n_estimators=200,          # Increase number of trees
    max_depth=10,              # Limit the depth of the trees
    min_samples_split=10,      # Set minimum samples to split a node
    min_samples_leaf=5,        # Set minimum samples at a leaf
    random_state=42
)

rf_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred_rf = rf_model.predict(X_test)
print("Random Forest R^2:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

# Evaluate the model
rf_r2 = r2_score(y_test, y_pred_rf)
rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
rf_mae = mean_absolute_error(y_test, y_pred_rf)

# Print evaluation metrics
print(f"Random Forest RÂ²: {rf_r2:.3f}")
print(f"Random Forest RMSE: {rf_rmse:.3f}")
print(f"Random Forest MAE: {rf_mae:.3f}")

"""#New Data Prediction"""

df.iloc[0].values.reshape(1,-1)
# Access the first row using iloc[0]
# Extract the values as a NumPy array using .values
# Reshape the array into a 2D array with one row using .reshape(1,-1)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming 'Posted On' is the first column and needs to be excluded
numerical_features = df.select_dtypes(include=['number']).columns  # Select only numerical columns

# Drop rows with NaN values if necessary:
numerical_df = df[numerical_features].dropna()

# Fit the scaler on the numerical data
scaler = StandardScaler()
scaler.fit(numerical_df)

# Transform the numerical data in the first row
transformed_data = scaler.transform(numerical_df.iloc[0].values.reshape(1, -1))

# If you need to include the 'Posted On' column in the output:
# Concatenate 'Posted On' with transformed data
# First, convert transformed_data to a pandas DataFrame for easier concatenation
transformed_df = pd.DataFrame(transformed_data, columns=numerical_features)
final_row = pd.concat([df[['Posted On']].iloc[0], transformed_df], axis=1)

print(final_row)

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Assuming 'Posted On' is the first column and needs to be excluded
numerical_features = df.select_dtypes(include=['number']).columns  # Select only numerical columns

# Drop rows with NaN values if necessary:
numerical_df = df[numerical_features].dropna()

# Fit the scaler on the numerical data, excluding the target variable
scaler = StandardScaler()
# Exclude the target variable column from scaling
scaler.fit(numerical_df.drop(columns=[target_variable_column]))

# Replace 'Rent' with the actual name of your target variable column
target_variable_column = 'Rent'

# Extract the target variable from the DataFrame
your_target_variable = numerical_df[target_variable_column]

regression = LinearRegression()  # Create a Linear Regression model

# Now use the extracted target variable in the fit method
regression.fit(numerical_df.drop(columns=[target_variable_column]), your_target_variable) # Train the model

# Ensure the input data for prediction has the same number of features as the training data
# Select all features used during training and scale them
input_data = numerical_df.drop(columns=[target_variable_column]).iloc[[0]]  # Select the first row as a DataFrame

# Scale the input data using the fitted scaler
scaled_input_data = scaler.transform(input_data)

# Predict using the scaled input data
prediction = regression.predict(scaled_input_data)

print(prediction)

"""#Pickle the Model file for Deployment"""

import pickle
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

# Save Linear Regression model
with open('models/lr_model.pkl', 'wb') as file:
    pickle.dump(lr_model, file)

# Save Random Forest model
with open('models/rf_model.pkl', 'wb') as file:
    pickle.dump(rf_model, file)

# Save LightGBM model
with open('models/lgb_model.pkl', 'wb') as file:
    pickle.dump(lgb_model, file)

# Save XGBoost model
with open('models/xgb_model.pkl', 'wb') as file:
    pickle.dump(xgb_model, file)

# Define and train the stacked_model
stacked_model = StackingRegressor(estimators=[('lr', lr_model), ('rf', rf_model)], final_estimator=LinearRegression())
stacked_model.fit(X_train, y_train)  # Use your training data here

# Save Stacked model
with open('models/stacked_model.pkl', 'wb') as file:
    pickle.dump(stacked_model, file)

# Example: Save feature columns during training
feature_columns = X.columns.tolist()
with open('models/feature_columns.pkl', 'wb') as f:
    pickle.dump(feature_columns, f)

import pickle

# Load Linear Regression model
with open('models/lr_model.pkl', 'rb') as file:
    lr_model = pickle.load(file)

# Load Random Forest model
with open('models/rf_model.pkl', 'rb') as file:
    rf_model = pickle.load(file)

# Load LightGBM model
with open('models/lgb_model.pkl', 'rb') as file:
    lgb_model = pickle.load(file)

# Load XGBoost model
with open('models/xgb_model.pkl', 'rb') as file:
    xgb_model = pickle.load(file)

# Load Stacked model
with open('models/stacked_model.pkl', 'rb') as file:
    stacked_model = pickle.load(file)

import pickle
import pandas as pd

# Assuming you have your trained Random Forest model loaded as 'rf_model'
# and your test data as 'X_test'

# 1. Make predictions on the test data
y_pred_rf = rf_model.predict(X_test)

# 2. Create a DataFrame to store the predictions
predictions_df = pd.DataFrame(y_pred_rf, columns=['Predicted_Rent'])

# 3. Save the predictions DataFrame to a pickle file
predictions_df.to_pickle('models/rf_predictions.pkl')

import pandas as pd

# Load the predictions from the pickle file
loaded_predictions = pd.read_pickle('rf_predictions.pkl')

# Access the predicted rent values
predicted_rent_values = loaded_predictions['Predicted_Rent']

